\chapter{Quantum Applications in Natural Language Processing}
\label{chap:qDisCo}

\begin{chapabstract}
        We propose a new application of quantum computing to the field of natural language processing (NLP).  Ongoing work in NLP attempts to incorporate grammatical structure into algorithms that compute meaning.  In \cite{clark2008compositional}, Clark et al. introduce such a model (the CCS model) that is based on tensor product composition. While this algorithm has many advantages, its implementation is hampered by the large classical computational resources that are required. In this work we show how computational shortcomings of the  CCS approach could be resolved using quantum computation. We address the value of a qRAM \cite{giovannetti2008quantum} for this model and extend an algorithm from Wiebe et al. \cite{wiebe2014quantum} into a quantum algorithm to categorize similar sentences in CCS. Our new algorithm demonstrates a speedup over classical methods under certain conditions.
\end{chapabstract}

\section{Introduction}

As human computer interfaces become more advanced, natural language processing has grown to be a ubiquitous part of our world.  Its techniques allow computers to understand natural language to perform tasks like automatic summarization, machine translation, information retrieval, and sentiment analysis. Most approaches to this problem, such as Google's search, understand strings of separate words in a `bag of words' approach, ignoring any grammatical structure. This is certainly unsatisfactory, as we know that the meaning of a sentence is more than the meaning of its component words. Research in \textit{distributional compositional semantics} (DisCo) seeks to address this by incorporating grammatical structure into bag-of-words models. 

In \cite{clark2008compositional}, Clark et al. introduce a DisCo model (the CCS model) based on tensor product composition that gives a grammatically informed algorithm to compute the meaning of sentences and phrases. In the framework of this thesis, their model lives in the oPT \cat{FVect}. While the algorithm has many advantages, its implementation is hampered by the large classical computational resources that it requires.  This paper presents ways that quantum computers can solve some of these problems, making the CCS model of linguistics an attractive application for quantum computation.

We use the fact that quantum computation is naturally suited to managing high dimensional tensor product spaces. Recent literature has shown that quantum algorithms can thus provide advantages for machine learning \cite{wiebe2014quantum,rebentrost2014quantum}, inference \cite{low2014quantum}, and regression \cite{wiebe2012quantum,wang2014quantum} tasks.  These results are leveraged in two particular ways:
\begin{enumerate}
\item We employ the scaling of quantum systems to address computational difficulties  inherent in tensor-product based compositional semantics.
\item Shared GCT structure makes algorithms in the CCS model especially amenable to quantum speedups.  We specify a CCS sentence similarity algorithm that, under certain conditions, gives quadratic speedups for natural language tasks.
\end{enumerate}
At an abstract level, we are taking a classical theory of NLP in the oPT \cat{FVect} and computing outcomes in the oPT of quantum theory $\cat{FHilb}$. The shared abstract structure makes the interface obvious.

In Section \ref{sec:disco}, we cover the basic framework of distributional compositional linguistics. Section 3 introduces the advantages of quantum representations for this framework.  Sections 4 and 5 propose a quantum algorithm with quadratic speedup for calculating sentence similarity within CCS. Section 6 briefly discusses the noise tolerance of these methods.

\section{Distributional Compositional Semantics and the CSS model}
\label{sec:disco}
In modern natural language processing, the \textbf{vector space model} is widely used to compute the meaning of individual words \cite{schutze1998automatic}. In this approach we first specify a set of context words, for example the 2000 most common words in a given corpus.  These context words then form the basis of the vector space of word meanings in the following manner: for some given word, say  ``quantum", we look through a corpus and count the frequency with which each basis word    appears `near' to ``quantum". It is likely that we would have a high frequency for words like ``physics" and ``information" for example.  These frequencies then form the \textbf{word vector} for ``quantum". Words are similar if the inner product of their word vectors is close to one. These `bag of words' methods are typically referred to as \textbf{distributional}.

As the same sentence rarely occurs repeatedly, this distributional technique cannot be directly extended to calculate the meaning of longer phrases, sentences, paragraphs, etc. Instead, \textbf{compositional} semantics designs algorithms for deriving the meaning of a sentence or phrase from known meanings of component words,  taking into account types and grammatical structure \cite{lambek2008word}. The \textbf{distributional compositional} semantic model (DisCo) combines both approaches to introduce grammatical form to the composition of word vectors \cite{clark2008compositional}. See Figure~\ref{fig:discofig}. 

\begin{figure}[t]
\begin{center}
\begin{tabular}{m{0.3\linewidth}@{}m{0.1\linewidth}@{}m{0.3\linewidth}}
 Distributional & & Compositional \\
 $\begin{aligned}
 \includegraphics[width=\linewidth]{distributional.png}
 \end{aligned}$
 & $\begin{aligned}\mbox{{\color{blue} \huge +}}\end{aligned}$ & 
  $\begin{aligned}
 \includegraphics[width=\linewidth]{compositional.png}
 \end{aligned}$
\end{tabular}
\end{center}
\caption[The DisCo approach.]{The DisCo approach combines word vectors with pregroup or combinatory categorical grammar. The diagram on the right shows which terms cancel in the derivation tree.  It is drawn suggestively as explained in Section \ref{sec:disco}. }
\label{fig:discofig}
\end{figure}

In this model, each grammatical type is assigned a tensor product space based on Lambek's pregroup grammar \cite{lambek2008word} or combinatorial categorical grammar \cite{hermann2013role}. The meaning of nouns is, for example, calculated as in the distributional case, and we label their vector space $\mathcal{N}$.  A transitive verb's meaning is then, following the grammar, a vector in the space $\mathcal{N}\otimes \mathcal{S} \otimes \mathcal{N}$, where $\mathcal{S}$ is the meaning space for sentences \cite{clark2008compositional}. An intuition for this is that the transitive verb takes a subject noun as a left argument and an object noun as a right argument. An adjective lives in the space $\mathcal{N}\otimes\mathcal{N}$.

The oPT diagrammatic notation for vectors, tensors, and linear maps is commonly used in CCS. Here vertical composition (read bottom to top) represents composition of linear maps and horizontal composition represents the tensor product:
\begin{align*}
\begin{tabu}{cc}
\overrightarrow{\mbox{Mary}}\in \mathcal{N} :=
\begin{aligned}
\begin{tikzpicture}[scale=0.7, yscale=-1]
                \node  (20) at (0, 0) {$\mathcal{N}$};
                \node  (31) at (0, 2.5) {Mary};
                \node (1) [state] at (0,1) {};
                \draw  (0,0.5) to (1.center);
\end{tikzpicture}
\end{aligned} & \qquad
f:\mathcal{M}\to\mathcal{N} :=
\begin{aligned}
\begin{tikzpicture}[scale=0.7]
                \node  (20) at (0, -1.5) {$\mathcal{M}$};
                \node  (20) at (0, 1.5) {$\mathcal{N}$};                
                \node (1) [morphism] at (0,0) {f};
                \draw  (0,1) to (0,0.5);   
                \draw  (0,-1) to (0,-0.5);
\end{tikzpicture}
\end{aligned} \\
\overrightarrow{\mbox{likes}}\in \mathcal{N}\otimes\mathcal{S}\otimes \mathcal{N} :=
\begin{aligned}
\begin{tikzpicture}[scale=0.7,yscale=-1]
                \node  (3) at (-1, 1) {};
                \node  (4) at (2, 1) {};
                \node  (5) at (0.5, 2.25) {};
                \node  (10) at (-0.25, 1) {};
                \node  (11) at (0.5, 1) {};
                \node  (12) at (1.25, 1) {};
                \node  (15) at (-0.25, 0.5) {};
                \node  (16) at (0.5, 0.5) {};
                \node  (17) at (1.25, 0.5) {};
                \node  (20) at (-0.25, 0) {$\mathcal{N}$};
                \node  (21) at (0.5, 0) {$\mathcal{S}$};
                \node  (22) at (1.25, 0) {$\mathcal{N}$};
                \node  (31) at (0.5, 2.75) {likes};
                \draw  (3.center) to (4.center);
                \draw (4.center) to (5.center);
                \draw (3.center) to (5.center);
                \draw (10.center) to (15.center);
                \draw (11.center) to (16.center);
                \draw (12.center) to (17.center);
\end{tikzpicture}
\end{aligned} 
 & \qquad
 \begin{aligned}
 \begin{tikzpicture}[yscale=1.4]
                \node (0) at (0, 1) {};
                \node (2) at (-1, 0) {$g\circ f = $};
                \node (1) at (0.5, 1) {$\mathcal{P}$};
                \node (3) at (0, -1) {};
                \node [style=morphism] (4) at (0, -0.5) {$f$};
                \node (5) at (0.5, -1) {$\mathcal{M}$};
                \node (6) at (0.5, 0) {$\mathcal{N}$};
                \node [style=morphism] (7) at (0, 0.5) {$g$};
                \draw (3.center) to (0, -0.75);
                \draw (0, -0.25) to (0, 0.25);
                \draw (0, 0.75) to (0.center);
    \end{tikzpicture}
    \end{aligned} \\
\overrightarrow{\mbox{Mary}}\otimes \overrightarrow{\mbox{likes}} :=
\begin{pic}[scale=0.7,yscale=-1]
                \node  (20) at (0, 0) {$\mathcal{N}$};
                \node  (31) at (0, 2.5) {Mary};
                \node (1) [state] at (0,1) {};
                \draw  (0,0.5) to (1.center);
                \node (2) [state, xscale=1.5, yscale=1] at (3,1) {};
                \node  (32) at (3, 2.5) {likes};
                \node  (20) at (2.25, 0) {$\mathcal{N}$};
                \node  (20) at (3, 0) {$\mathcal{S}$};
                \node  (20) at (3.75, 0) {$\mathcal{N}$};
                \draw  (3,0.5) to (2.center);
                \draw  (3.75,0.5) to (3.75,1);
                \draw  (2.25,0.5) to (2.25,1);    
\end{pic} & \qquad\qquad
\sum\limits_i \langle ii| :=
\begin{pic}[scale=0.7, yscale=-1]
                \node  (0) at (-1, 1.5) {$\mathcal{N}$};
                \node  (1) at (1, 1.5) {$\mathcal{N}$};
                \node  (0) at (-1, 1) {};
                \node  (1) at (1, 1) {};
                \node  (2) at (0, 0) {};
                \draw [bend right=45, looseness=1.00] (0.center) to (2.center);
                \draw [bend right=45, looseness=1.00] (2.center) to (1.center);               
\end{pic}
\end{tabu}
\end{align*}
where $f:\mathcal{N}\to\mathcal{M}$ and $g:\mathcal{M}\to\mathcal{P}$ are linear maps and the linear map $\sum_i\langle ii|$ sums over all the basis vectors of $\mathcal{N}$ and is the usual oPT cap.

Given a well-typed sentence with meaning vectors $\vec{w_j}$ for each of $k$ words, the classical CCS algorithm for calculating a sentence's meaning is \cite{clark2013quantum}:
\begin{enumerate}
\item Compute the tensor product $\overrightarrow{\mbox{words}}=\vec{w_0}\otimes...\otimes \vec{w_k}$ in the order that each word appears in the sentence.

\item Construct a linear map that represents the type reduction by ``wiring up" the vectors with the appropriate caps as in the following example:

\begin{equation}
\label{eq:svo}
\begin{aligned}
\begin{tikzpicture}[scale=0.5, yscale=-1]     
                \node (0) at (-3.5, 1) {};
                \node (1) at (-2, 1) {};
                \node (2) at (-2.75, 2) {};
                \node (3) at (-1, 1) {};
                \node (4) at (2, 1) {};
                \node (5) at (0.5, 2.25) {};
                \node (6) at (4.5, 1) {};
                \node (7) at (3.75, 2) {};
                \node (8) at (3, 1) {};
                \node (9) at (-2.75, 1) {};
                \node (10) at (-0.25, 1) {};
                \node (11) at (0.5, 1) {};
                \node (12) at (1.25, 1) {};
                \node (13) at (3.75, 1) {};
                \node (14) at (-2.75, 0.5) {};
                \node (15) at (-0.25, 0.5) {};
                \node (16) at (0.5, 0.5) {};
                \node (17) at (1.25, 0.5) {};
                \node (18) at (3.75, 0.5) {};
                \node (19) at (-2.75, 0) {$N$};
                \node (20) at (-0.25, 0) {$N$};
                \node (21) at (0.5, 0) {$S$};
                \node (22) at (1.25, 0) {$N$};
                \node (23) at (3.75, 0) {$N$};
                \node (24) at (-2.75, -0.5) {};
                \node (25) at (-0.25, -0.5) {};
                \node (26) at (0.5, -0.5) {};
                \node (27) at (1.25, -0.5) {};
                \node (28) at (3.75, -0.5) {};
                \node (29) at (0.5, -1.75) {};
                \node (30) at (-2.75, 2.75) {Mary};
                \node (31) at (0.5, 2.75) {likes};
                \node (32) at (3.75, 2.75) {words.};
                \draw (0.center) to (1.center);
                \draw (1.center) to (2.center);
                \draw (2.center) to (0.center);
                \draw  (3.center) to (4.center);
                \draw (8.center) to (6.center);
                \draw (6.center) to (7.center);
                \draw (4.center) to (5.center);
                \draw (3.center) to (5.center);
                \draw (8.center) to (7.center);
                \draw (9.center) to (14.center);
                \draw (10.center) to (15.center);
                \draw (11.center) to (16.center);
                \draw (12.center) to (17.center);
                \draw (13.center) to (18.center);
                \draw [thick, bend right=90, looseness=1.25] (24.center) to (25.center);
                \draw [thick, bend right=90, looseness=1.25] (27.center) to (28.center);
                \draw (26.center) to (29.center);
\end{tikzpicture}
\end{aligned}
\end{equation}

\item Compute the meaning of the sentence by applying the linear map to the $\overrightarrow{\mbox{words}}$ vector. This results in a vector in $\mathcal{S}$ which corresponds to the meaning of the sentence.
\end{enumerate}

\noindent We refer the reader to \cite{coecke2010mathematical} for a fuller description of the distributional compositional model and to \cite{experimental-catcompdist} and \cite{kartsaklis2012unified} for experimental implementations.

These models suggest a promising approach to incorporate grammatical structure with vector space models of meaning, yet they come with the computational challenges of large tensor product spaces \cite{GrefenstetteThesis2013}. While there do exist some classical approaches to avoid the calculation of the full tensor product, such as the holographic reduced representations from \cite{plate1991holographic} or the use of dimensionality reduction \cite{polajnar2013learning}, these approaches introduce further assumptions and inaccuracies.  For this reason, ameliorating the large computational costs introduced these large spaces through quantum computation is of particular interest.

\section{Quantum computation for the CCS model}

The most immediate advantage for quantum implementations of the CCS model is gained by storing meaning vectors in quantum systems.  For $\alpha,\beta\in \mathbb{C}$ a two-level quantum system, a qubit, is defined by:
\begin{center}
  \begin{tabular}{cc}
   Qubit & Qubits  \\
   $\begin{array}{lcl}
        \ket{\psi} &=& \alpha\ket{0}+\beta\ket{1} \\[0.2cm]
        &=&\alpha\left(\begin{array}{c} 1 \\ 0 \end{array}\right)
            +\beta\left(\begin{array}{c} 0 \\ 1 \end{array}\right)
   \end{array} $
   &\qquad\qquad
   $\ket{\psi_1}\otimes\ket{\psi_2} = \left(\begin{array}{c} \alpha_1\alpha_2 \\ \alpha_1\beta_2\\\beta_1\alpha_2\\\beta_2\beta_2 \end{array}\right)$
   \\
  \end{tabular}
\end{center}
and composition of qubits is given by the tensor product.  This leaves each $n$-qubit system with $2^n$ degrees of freedom, indicating that $N$-dimensional classical vectors can be stored in $\log_2 N$ qubits.
Consider a corpus whose word-meaning space is given by a basis of the 2,000 most common words. Even if we make the simplifying assumption that the sentence-meaning space is no larger than the word-meaning space we obtain the dramatic improvements details in Table 1.

\begin{table}[t]
\label{tab:space}
\begin{center}
\begin{tabular}{|c|c|c|}\hline
 & One Transitive Verb & 10k tr. verbs \\\hline
 Classical & $8\times 10^{9}$ bits & $8\times 10^{13}$ bits \\\hline
 Quantum & 33 qubits & 47 qubits \\\hline
\end{tabular}
\end{center}
\caption{Rough comparisons of the storage necessary for verbs in quantum and classical frameworks.}
\end{table}

Further, these word meanings can be imported into a ``bucket bridgade" quantum RAM that allows them to retrieved with a complexity linear in the number of qubits~\cite{giovannetti2008quantum}. The general point is that because quantum systems compose via the tensor product they are a natural choice to store complex types and sentences that have the same compositional structure. We can then employ quantum algorithms on for natural language classification as presented in Section \ref{sec:discoQalg}.

\section{A quantum algorithm for the closest vector problem}
\label{sec:qalg}

Many tasks in computational linguistics such as clustering, text classification, phrase/word similarity, and sentiment analysis rely on computations that determine the closest vector to $\vec{s}$ out of some set of $N$-dimensional vectors $\{\vec{v}_0,\vec{v}_1,...\vec{v}_{M-1}\}$. In clustering algorithms, for example, the set of vectors could be either the centroids of different clusters or the full set of training vectors, as in nearest neighbor clustering algorithms. Either the inner-product distance or Euclidean distance can be used. We will assume that all vectors are $N$-dimensional.
\begin{defn}
Given vector $\vec{s}$ and a set of $M$ vectors $U = \{\vec{v}_0,\vec{v}_1,...\vec{v}_{M-1}\}$ the \textbf{closest vector problem} asks one to determine which $v_j$ has the smallest inner product distance with $\vec{s}$.
\end{defn}
Direct calculation of the smallest vector would have complexity $\mathcal{O}(MN)$.  In \cite{wiebe2014quantum} the authors introduce a quantum algorithm for this problem that, under certain conditions, demonstrate quadratic speedups over direct calculation and polynomial speedups over Monte-Carlo methods. Some proof of principle experiments have then demonstrated clustering of eight-dimensional vectors, based on these techniques, on a small photonic quantum computer \cite{cai2015entanglement}. This algorithm requires the following assumptions, where we write $v_{ji}$ for the $i^{\tiny\mbox{th}}$ entry of the $j^{\tiny\mbox{th}}$ vector:
\begin{enumerate}
\item Vectors $\vec{v_j}$ and $\vec{s}$ are $d$-sparse, with no more than $d$ non-zero entries.
\item The relevant vectors are stored in some kind of quantum memory so that the quantum computer can access their entries with the two oracles of the form:
\begin{equation}
\begin{array}{c}
\mathcal{O}\ket{j}\ket{i}\ket{0}:=\ket{j}\ket{i}\ket{v_{ji}}, \\
\mathcal{F}\ket{j}\ket{l}:= \ket{j}\ket{f(j,l)},
\end{array}
\end{equation}
where $f(j,l)$ is the location of the $l^{\tiny\mbox{th}}$ non-zero entry of $v_j$.  It is against these memory access oracles that the performance of our algorithm will be measured.

\item $\max(|v_{ji}|^2)\le r_{\small\mbox{max}}$ for some known constant $r_{\small\mbox{max}}$.

\item All the vectors are normalized.
 
\end{enumerate}

Under these assumptions we are able to run a quantum nearest-neighbor algorithm with complexity characterized by the following theorem:

\begin{theorem}[\cite{wiebe2014quantum}]
We can find $\max_j|\langle s~|~v_j\rangle|^2$ with success probability $1-\delta$ and error $\epsilon$ using an expected number of $\mathcal{O}$ and $\mathcal{F}$ queries that is bounded above by
\begin{equation}
1080\sqrt{M}\left\lceil\frac{4\pi(\pi+1)d^2r^4_{\tiny\mbox{max}}}{\epsilon}\right\rceil\left\lceil \frac{\ln\left(81M(\ln(M)+\gamma)\right)/\delta_0}{2(8/\pi^2-1/2)^2}\right\rceil,
\end{equation}
where $\gamma\approx 0.5772$ is Euler's constant.
\end{theorem}

It is clear that for this quantum algorithm there is a quadratic improvement in scaling with $M$, the number of training vectors. While the dimension of the vectors $N$ is not explicitly included, in general it is implicitly there through the dependence on $d$.  It is also clear that if $r_{\small\mbox{max}}\propto 1/\sqrt{d}$, then the algorithm's dependence on both $d$ and $N$ drops out. As the vectors are normalized, this can be expected to occur if the vectors have sparsity that grows linearly with their size \cite{wiebe2014quantum}. The authors further assume that for ``typical" cases the error $\epsilon$ scales as $\Theta(1/\sqrt{N})$ so that the runtime for the quantum inner-product algorithm becomes $\mathcal{O}\left(\sqrt{NM}\ln(M)d^2r^4_{\small\mbox{max}}\right)$.\footnote{
This is argued for in Appendix D of \cite{wiebe2014quantum} for a ``typical" case where the vectors are uniformly distributed over the unit sphere.} This result shows a quadratic improvement over direct calculations and also shows improvement over Monte Carlo methods, whose complexity is $\mathcal{O}\left(NMd^2r^4_{\small\mbox{max}}\right)$. These comparisons are summarized in Table 2.

\begin{table}[ht]
\label{tab:comp}
\begin{center}
\begin{tabular}{|c|c|c|}\hline
 Type & Typical cases & Atypical cases    \\ \hline
 Classical Direct & $\mathcal{O}(NM)$ & $\mathcal{O}(NM)$ \\
 Classical Monte Carlo & $\mathcal{O}\left(NMd^2r_{\small\mbox{max}}^4\right)$ & $\mathcal{O}\left(Md^2r_{\small\mbox{max}}^4/\epsilon^2\right)$ \\
 Quantum & $\mathcal{O}\left(\sqrt{NM}\log(M)d^2r^4_{\small\mbox{max}}\right)$ &  $\mathcal{O}\left(\sqrt{M}\log(M)d^2r^4_{\small\mbox{max}}/\epsilon\right)$ \\\hline
\end{tabular}
\end{center}
\caption{Complexity comparisons for different closest vector algorithms. Adapted from \cite{wiebe2014quantum}.}
\end{table}

 In the following section we adapt this algorithm to sentence similarity calculations in the distributional compositional framework.

\section{A quantum algorithm for CCS sentence similarity}
\label{sec:discoQalg}

The quantum algorithm from Section \ref{sec:qalg} can be used to advantage in natural language processing tasks however, the computational overhead of the CCS approach would dwarf this algorithm's advantages if it were naively applied.  
Throughout this section we will assume both that $r_{\small\mbox{max}}\propto 1/\sqrt{d}$ and that the accuracy necessary for our calculation means $\epsilon$ scales as $\Theta(1/\sqrt{N})$. Consider the example of probabilistically classifying the meaning of a  simple sentence. We illustrate this example with a noun-verb-noun sentence. The meaning of the nouns are vectors in an $N$-dimensional vector space and the meaning of the verb is a vector in the space $N\otimes S \otimes N$. We represent a derivation of the meaning of the full sentence with the following map:

\begin{equation}
\label{eqn:phi}
\begin{aligned}
\begin{tikzpicture}[scale=0.6, yscale=-1]
                \node (0) at (-5.5, 0) {\ket{\phi}};
                \node (0) at (-4.5, 0) {=};       
                \node (0) at (-3.5, 1) {};
                \node (1) at (-2, 1) {};
                \node (2) at (-2.75, 2) {};
                \node (3) at (-1, 1) {};
                \node (4) at (2, 1) {};
                \node (5) at (0.5, 2.25) {};
                \node (6) at (4.5, 1) {};
                \node (7) at (3.75, 2) {};
                \node (8) at (3, 1) {};
                \node (9) at (-2.75, 1) {};
                \node (10) at (-0.25, 1) {};
                \node (11) at (0.5, 1) {};
                \node (12) at (1.25, 1) {};
                \node (13) at (3.75, 1) {};
                \node (14) at (-2.75, 0.5) {};
                \node (15) at (-0.25, 0.5) {};
                \node (16) at (0.5, 0.5) {};
                \node (17) at (1.25, 0.5) {};
                \node (18) at (3.75, 0.5) {};
                \node (19) at (-2.75, 0) {$N$};
                \node (20) at (-0.25, 0) {$N$};
                \node (21) at (0.5, 0) {$S$};
                \node (22) at (1.25, 0) {$N$};
                \node (23) at (3.75, 0) {$N$};
                \node (24) at (-2.75, -0.5) {};
                \node (25) at (-0.25, -0.5) {};
                \node (26) at (0.5, -0.5) {};
                \node (27) at (1.25, -0.5) {};
                \node (28) at (3.75, -0.5) {};
                \node (29) at (0.5, -1.75) {};
                \node (30) at (-2.75, 2.75) {kids};
                \node (31) at (0.5, 2.75) {play};
                \node (32) at (3.75, 2.75) {football};
                \draw (0.center) to (1.center);
                \draw (1.center) to (2.center);
                \draw (2.center) to (0.center);
                \draw  (3.center) to (4.center);
                \draw (8.center) to (6.center);
                \draw (6.center) to (7.center);
                \draw (4.center) to (5.center);
                \draw (3.center) to (5.center);
                \draw (8.center) to (7.center);
                \draw (9.center) to (14.center);
                \draw (10.center) to (15.center);
                \draw (11.center) to (16.center);
                \draw (12.center) to (17.center);
                \draw (13.center) to (18.center);
                \draw [thick, bend right=90, looseness=1.25] (24.center) to (25.center);
                \draw [thick, bend right=90, looseness=1.25] (27.center) to (28.center);
                \draw (26.center) to (29.center);
\end{tikzpicture}
\end{aligned}
\end{equation}

From now on, we will take sentences to exist in the same meaning space as words, i.e. $S\simeq N$.

\begin{defn}
For meaning vector $\vec{s}$ and $M$ sets of meaning vectors, a \textbf{classification task} assigns $\vec{s}$ to the set containing the nearest-neighbor of $\vec{s}$.
\end{defn}

An example task would be to determine if a sentence is about sports or politics or if a sentence expresses agreement or disagreement. 
If, to present a simplified example, we take each cluster to only contain a single vector ($\vec{v}$ and $\vec{w}$ respectively) then the sentence would be classified by computing 
\begin{equation}
\label{eq:exampleCalcs}
\begin{aligned}
\begin{tikzpicture}[scale=0.5, yscale=-1]
                \node (0) at (-3.5, 1) {};
                \node (1) at (-2, 1) {};
                \node (2) at (-2.75, 2) {};
                \node (3) at (-1, 1) {};
                \node (4) at (2, 1) {};
                \node (5) at (0.5, 2.25) {};
                \node (6) at (4.5, 1) {};
                \node (7) at (3.75, 2) {};
                \node (8) at (3, 1) {};
                \node (9) at (-2.75, 1) {};
                \node (10) at (-0.25, 1) {};
                \node (11) at (0.5, 1) {};
                \node (12) at (1.25, 1) {};
                \node (13) at (3.75, 1) {};
                \node (14) at (-2.75, 0.5) {};
                \node (15) at (-0.25, 0.5) {};
                \node (16) at (0.5, 0.5) {};
                \node (17) at (1.25, 0.5) {};
                \node (18) at (3.75, 0.5) {};
                \node (19) at (-2.75, 0) {$N$};
                \node (20) at (-0.25, 0) {$N$};
                \node (21) at (0.5, 0) {$S$};
                \node (22) at (1.25, 0) {$N$};
                \node (23) at (3.75, 0) {$N$};
                \node (24) at (-2.75, -0.5) {};
                \node (25) at (-0.25, -0.5) {};
                \node (26) at (0.5, -0.5) {};
                \node (27) at (1.25, -0.5) {};
                \node (28) at (3.75, -0.5) {};
                \node (29) at (0.5, -1.75) {};
                \node (30) at (-2.75, 2.75) {kids};
                \node (31) at (0.5, 2.75) {play};
                \node (32) at (3.75, 2.75) {football};
                \node [style=state, hflip] (33) at (0.5, -1.75) {$\vec{v}$};
                \node (34) at (0.6, -3.5) {sports};
                \draw (0.center) to (1.center);
                \draw (1.center) to (2.center);
                \draw (2.center) to (0.center);
                \draw  (3.center) to (4.center);
                \draw (8.center) to (6.center);
                \draw (6.center) to (7.center);
                \draw (4.center) to (5.center);
                \draw (3.center) to (5.center);
                \draw (8.center) to (7.center);
                \draw (9.center) to (14.center);
                \draw (10.center) to (15.center);
                \draw (11.center) to (16.center);
                \draw (12.center) to (17.center);
                \draw (13.center) to (18.center);
                \draw [thick, bend right=90, looseness=1.25] (24.center) to (25.center);
                \draw [thick, bend right=90, looseness=1.25] (27.center) to (28.center);
                \draw (26.center) to (29.center);
\end{tikzpicture}
\end{aligned}
\qquad \mbox{and} \qquad
\begin{aligned}
\begin{tikzpicture}[scale=0.5, yscale=-1]
                \node (0) at (-3.5, 1) {};
                \node (1) at (-2, 1) {};
                \node (2) at (-2.75, 2) {};
                \node (3) at (-1, 1) {};
                \node (4) at (2, 1) {};
                \node (5) at (0.5, 2.25) {};
                \node (6) at (4.5, 1) {};
                \node (7) at (3.75, 2) {};
                \node (8) at (3, 1) {};
                \node (9) at (-2.75, 1) {};
                \node (10) at (-0.25, 1) {};
                \node (11) at (0.5, 1) {};
                \node (12) at (1.25, 1) {};
                \node (13) at (3.75, 1) {};
                \node (14) at (-2.75, 0.5) {};
                \node (15) at (-0.25, 0.5) {};
                \node (16) at (0.5, 0.5) {};
                \node (17) at (1.25, 0.5) {};
                \node (18) at (3.75, 0.5) {};
                \node (19) at (-2.75, 0) {$N$};
                \node (20) at (-0.25, 0) {$N$};
                \node (21) at (0.5, 0) {$S$};
                \node (22) at (1.25, 0) {$N$};
                \node (23) at (3.75, 0) {$N$};
                \node (24) at (-2.75, -0.5) {};
                \node (25) at (-0.25, -0.5) {};
                \node (26) at (0.5, -0.5) {};
                \node (27) at (1.25, -0.5) {};
                \node (28) at (3.75, -0.5) {};
                \node (29) at (0.5, -1.75) {};
                \node (30) at (-2.75, 2.75) {kids};
                \node (31) at (0.5, 2.75) {play};
                \node (32) at (3.75, 2.75) {football};
                \node [style=state, hflip] (33) at (0.5, -1.75) {$\vec{w}$};
                \node (34) at (0.6, -3.5) {politics};
                \draw (0.center) to (1.center);
                \draw (1.center) to (2.center);
                \draw (2.center) to (0.center);
                \draw  (3.center) to (4.center);
                \draw (8.center) to (6.center);
                \draw (6.center) to (7.center);
                \draw (4.center) to (5.center);
                \draw (3.center) to (5.center);
                \draw (8.center) to (7.center);
                \draw (9.center) to (14.center);
                \draw (10.center) to (15.center);
                \draw (11.center) to (16.center);
                \draw (12.center) to (17.center);
                \draw (13.center) to (18.center);
                \draw [thick, bend right=90, looseness=1.25] (24.center) to (25.center);
                \draw [thick, bend right=90, looseness=1.25] (27.center) to (28.center);
                \draw (26.center) to (29.center);
\end{tikzpicture}
\end{aligned}
\end{equation}
and assigning the sentence to the one of smaller value.  We would proceed with two steps:

\begin{enumerate}
\item Compute the derivation of $\ket{\phi}$, which, by classical direct calculation, takes  $\mathcal{O}(3N)$ operations. 

\item See which of $\vec{v}$ and $\vec{w}$ is closest to $\ket{\phi}$. This is an instance of the closest vector problem where $\vec{s} = \ket{\phi}$, $M=2$, and $U = \{\vec{v},\vec{w}\}$. With direct calculation or Monte Carlo the second step requires\footnote{If we assume the appropriate $d$-sparsity scaling.} $\mathcal{O}(2N)$ to be compared with the quantum method at $\mathcal{O}(\sqrt{2N}\log 2)$. Even if we include the step to import the classical data from step one into quantum form, which can be done with $\mathcal{O}(\log_2N)$ overhead \cite{giovannetti2008quantum}, then we obtain a speedup for this step. 
\end{enumerate}

Still, despite the quantum speedup from step two, the full algorithm for general $M$ runs in $\mathcal{O}(3N\sqrt{M}\log M)$, remaining linear in $N$.

In order to recover a speedup we refine the application of the quantum algorithm by posing a version of the closest vector problem that avoids the initial calculation of $\ket{\phi}$ altogether. Note the equivalence of the calculations in Equation \ref{eq:exampleCalcs} with 
\begin{equation}
\label{eqn:trick}
\begin{aligned}
\begin{tikzpicture}[scale=0.5, yscale=-1]
                \node (3) at (-3, 1) {};
                \node (4) at (4, 1) {};
                \node (5) at (0.5, 2.25) {};
                \node (10) at (-2, 1) {};
                \node (11) at (0.5, 1) {};
                \node (12) at (3, 1) {};
                \node (13) at (3.75, 1) {};
                \node (14) at (-2.75, 0.5) {};
                \node (15) at (-2, 0.5) {};
                \node (16) at (0.5, 0.5) {};
                \node (17) at (3, 0.5) {};
                \node (18) at (3.75, 0.5) {};
                \node (20) at (-2, 0) {$N$};
                \node (21) at (0.5, 0) {$S$};
                \node (22) at (3, 0) {$N$};
                \node (24) at (-2.75, -0.5) {};
                \node (25) at (-0.25, -0.5) {};
                \node (26) at (0.5, -0.5) {};
                \node (27) at (1.25, -0.5) {};
                \node (28) at (3.75, -0.5) {};
                \node (29) at (0.5, -1.75) {};
                \node (30) at (-2, -3.5) {kids};
                \node (31) at (0.5, 2.75) {play};
                \node (32) at (3.5, -3.5) {football};
                \node [style=state, hflip] (33) at (-2, -1.75) {};
                \node [style=state, hflip] (35) at (0.5, -1.75) {};
                \node [style=state, hflip] (36) at (3, -1.75) {};
                \node (34) at (0.6, -3.5) {sports};
                \draw  (3.center) to (4.center);
                \draw (4.center) to (5.center);
                \draw (3.center) to (5.center);
                \draw (10.center) to (15.center);
                \draw (11.center) to (16.center);
                \draw (12.center) to (17.center);
                \draw (26.center) to (29.center);
                \draw (-2,-0.5) to (33.center);
                \draw (3,-0.5) to (36.center);
\end{tikzpicture}
\end{aligned}
\qquad \mbox{and} \qquad
\begin{aligned}
\begin{tikzpicture}[scale=0.5, yscale=-1]
                \node (3) at (-3, 1) {};
                \node (4) at (4, 1) {};
                \node (5) at (0.5, 2.25) {};
                \node (10) at (-2, 1) {};
                \node (11) at (0.5, 1) {};
                \node (12) at (3, 1) {};
                \node (13) at (3.75, 1) {};
                \node (14) at (-2.75, 0.5) {};
                \node (15) at (-2, 0.5) {};
                \node (16) at (0.5, 0.5) {};
                \node (17) at (3, 0.5) {};
                \node (18) at (3.75, 0.5) {};
                \node (20) at (-2, 0) {$N$};
                \node (21) at (0.5, 0) {$S$};
                \node (22) at (3, 0) {$N$};
                \node (24) at (-2.75, -0.5) {};
                \node (25) at (-0.25, -0.5) {};
                \node (26) at (0.5, -0.5) {};
                \node (27) at (1.25, -0.5) {};
                \node (28) at (3.75, -0.5) {};
                \node (29) at (0.5, -1.75) {};
                \node (30) at (-2, -3.5) {kids};
                \node (31) at (0.5, 2.75) {play};
                \node (32) at (3.5, -3.43) {football};
                \node [style=state, hflip] (33) at (-2, -1.75) {};
                \node [style=state, hflip] (35) at (0.5, -1.75) {};
                \node [style=state, hflip] (36) at (3, -1.75) {};
                \node (34) at (0.6, -3.54) {politics};
                \draw (3.center) to (4.center);
                \draw (4.center) to (5.center);
                \draw (3.center) to (5.center);
                \draw (10.center) to (15.center);
                \draw (11.center) to (16.center);
                \draw (12.center) to (17.center);
                \draw (26.center) to (29.center);
                \draw (-2,-0.5) to (33.center);
                \draw (3,-0.5) to (36.center);
\end{tikzpicture}
\end{aligned}
\end{equation}
Rather than directly calculating $\ket{\phi}$, which is not relevant to the classification task, we can formulate a closest vector problem where $\vec{s} = \ket{play}$, $M=2$ and $U = \{\ket{kids}\otimes\ket{v} \otimes\ket{football},\ket{kids}\otimes\ket{w} \otimes\ket{football}\}$.
The runtime of this \textbf{deferred quantum algorithm}, including import, will then be $\mathcal{O}(\sqrt{MN})$, showing our desired quadratic speedup in both variable.

We extend this to result to include general sentences in the CCS model with the following theorem.\footnote{The authors would like to thank an anonymous reviewer for correcting an error in the original statement of the algorithm.}

\begin{theorem}
For an $N$-dimensional noun meaning space, there exists a quantum algorithm to classify any CCS model sentence composed of $n$ tensors $\vec{w_0},\vec{w_1},...,\vec{w_{n-1}}$ whose maximum dimension is $N^k$ into $M$ classes with time $\mathcal{O}(\sqrt{M N^{k(n-1)/2}} \log M)$. This is not always an improvement on the classical algorithm which runs in $\mathcal{O}(MN)$, but will be if $k(n-1) < 4$ (very short sentence fragments) or if $M$ is much larger than $N$, thought this lasts case is unlikely in practice.
\end{theorem}
\begin{proof}
The trick we play in Equation \ref{eqn:trick} amounts to splitting the sentence derivation into a bipartite graph.  As the CCS connections are based on a pregroup derivation, the connections will always form a tree, taking words as nodes and connections as edges. Trees can always be partitioned into bipartite graphs, thus, up to the ordering of inputs on each tensor which can be kept track of, we can always give a deferred quantum algorithm with associated speedup for any such CCS sentence.
 The following procedure explicitly details how to construct this bipartite partitioning.

For every CCS sentence there is one word that acts as the \textbf{head} of the derivation.  This is the word whose output $S$ wire contains the sentence meaning following its derivation's linear map. In Equation \ref{eqn:phi} this is the word ``play". Connect the dangling wire of the head word $\vec{w_h}$ with the vector $\vec{v_i}$ against which similarity is being measured.  Starting with this head word we then separate the sentence into a top layer and a bottom layer with the following steps.  Assign the head word to the top layer. Place every word it is connected to on the bottom layer. Next for every word just assigned to the bottom, take all their connected words, which are not yet assigned, and assign them to the top.  Continue this procedure while alternating top and bottom until all words are assigned. This gives a simple two-coloring of the derivation graph. 
\end{proof}

\begin{example}
Consider the following example sentence from \cite{dimitriThesis}:
\begin{equation*}
\begin{tikzpicture}[scale=1, yscale=-1]
                \node [style=state] (0) at (-6, 1) {1};
                \node (1) at (-5.25, 0.25) {};
                \node [style=state] (2) at (-2, 1) {2};
                \node [style=state] (3) at (2, 1) {2};
                \node [style=state] (4) at (3.5, 1) {3};
                \node (5) at (-1.25, 0.25) {};
                \node (6) at (1.25, 0.25) {};
                \node (7) at (2.75, 0.25) {};
                \node (8) at (-4.5, 1) {};
                \node (9) at (-3.5, 1) {};
                \node (10) at (-0.5, 1) {};
                \node (11) at (0.5, 1) {};
                \node (label1) at (0, 1.175) {1};
                \node [style=state, xscale=2] (12) at (0, 1) {};
                \node (label1) at (-4, 1.175) {0};
                \node [style=state, xscale=2] (13) at (-4, 1) {};
                \node (14) at (-4, -1) {};
                \node (15) at (-1.75, -0.5) {};
                \node (16) at (-6, 1.9) {John};
                \node (17) at (-4, 1.9) {saw};
                \node (18) at (-2, 1.9) {Mary};
                \node (19) at (0, 1.9) {read};
                \node (20) at (2, 1.9) {a};
                \node (21) at (3.5, 1.9) {book};
                \draw [bend right=45, looseness=1.00] (0) to (1.center);
                \draw [bend right=45, looseness=1.00] (2) to (5.center);
                \draw [in=-90, out=0, looseness=1.00] (7.center) to (4);
                \draw [bend left=45, looseness=1.00] (8.center) to (1.center);
                \draw (13) to (14.center);
                \draw [bend left=45, looseness=1.00] (10.center) to (5.center);
                \draw [bend right=45, looseness=1.00] (9.center) to (15.center);
                \draw [bend right=45, looseness=1.00] (11.center) to (6.center);
                \draw [bend right=45, looseness=1.00] (6.center) to (1.9,1);
                \draw [bend right=45, looseness=1.00] (2.1,1) to (7.center);
                \draw [bend right=45, looseness=1.00] (15.center) to (12);
\end{tikzpicture}
\end{equation*}
where we have labeled the vectors based on their depth in the derivation tree.  The two-layer form assigns even vectors to the top layer and odd vectors to the bottom:
\begin{equation*}
\begin{tikzpicture}[scale=1, yscale=-1]
                \node [style=state, hflip] (0) at (-5, 0) {};
                \node [style=state] (1) at (-2.5, 1) {};
                \node [style=state] (2) at (-1.25, 1) {};
                \node [style=state, hflip] (3) at (-0.75, 0) {};
                \node (4) at (-4.5, 1) {};
                \node (5) at (-3.5, 1) {};
                \node (6) at (-2.5, -1) {};
                \node (7) at (-2, 0) {};
                \node [style=state, hflip, xscale=2] (8) at (-2.5, 0) {};
                \node [style=state, xscale=2] (9) at (-4, 1) {};
                \node (10) at (-4, -1) {};
                \node (11) at (-5, -0.75) {John};
                \node (12) at (-4, 1.75) {saw};
                \node (13) at (-2.5, 1.75) {Mary};
                \node (14) at (-2.5, -0.75) {read};
                \node (15) at (-1.25, 1.75) {a};
                \node (16) at (-0.75, -0.75) {book};
                \draw (9) to (10.center);
                \draw (4.center) to (0);
                \draw (1) to (-2.5,0);
                \draw (7.center) to (-1.4,1);
                \draw (-1,1) to (3);
                \draw (5.center) to (-3,0);
\end{tikzpicture}
\end{equation*}
\end{example}

\noindent Hooking the dangling wire up to a classifying vector reduces the similarity computation to the calculation of a single inner product. Note that this procedure works for any derivation tree,\footnote{Even non-pregroup and non-CCG models will work as long as there is some tree derivation.} so sentence fragments, such as noun phrases, can be easily analyzed in exactly the same manner. 

It is also very likely that other algorithms can be found, which decompose the sentence in other ways, and that give other time complexities.

\section{Noise tolerance and Conclusion}

To reap the benefits of quantum algorithm in the domain of natural language processing, we  apply a new technique to defer the calculation of a sentence's compositional meaning, eliminating the overhead costs. By this method we are able to introduce a quantum algorithm for calculating sentence similarity that offers quadratic speedup over classical direct calculation and Monte-Carlo methods. These kinds of algorithms are particularly attractive for practical applications of quantum computing as noisy results can be tolerated: in our case when the desired errors is lower bounded by $1/\sqrt{N}$.  Vector space models are already inherently noisy and typical tasks allow for errors in results, so this restriction does not affect their efficacy. 

An additional point is that the density matrix formalism of \cite{piedeleu2015open} can also be naturally modeled by mixed states of quantum systems.  In fact, this analogy was the genesis for the theory of disambiguation presented there, as another example of the shared structure that led to the results presented here. At a basic level, our work exploits the abstract connection between natural language processing and quantum information.  More formally,  we can see both quantum computation in the category of finite dimensional Hilbert spaces and linear maps \cite{abramsky2004categorical,cqm-notes} and CCS in the product category of pregroup grammar and finite dimensional vectors spaces \cite{coecke2010mathematical}. The connection between these two (as process theories) makes the application of one to the other apparent.
